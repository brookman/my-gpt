# MyGPT
Implement a minimal Generative Pre-trained Transformer model (GPT) in Rust.
- Based on [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy.
- Original repo:  https://github.com/karpathy/ng-video-lecture
- Using the [huggingface/candle crate](https://github.com/huggingface/candle)

## Progress
- [x] Tokenization / encoding
- [x] Version 1: Train & evaluate basic Bigram model
- [ ] Version 2: using matrix multiply
- [ ] Version 3: adding softmax
- [ ] Version 4: self-attention
- [ ] ...
